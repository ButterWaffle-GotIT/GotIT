{
	"id": 4114,
	"slug": "prompt-injection",
	"term": { "ko": "프롬프트 인젝션", "en": "Prompt Injection" },
	"aliases": ["프롬프트 주입 공격"],
	"summary": "악의적인 사용자가 LLM의 작동 방식을 재정의하거나 내부 명령을 노출하도록 조작하는 텍스트 기반 공격 기법.",
	"onelinerForNonTech": "AI에게 '너는 이제부터 내 말만 들어야 해'라는 비밀 명령을 몰래 주입하여, AI가 원래의 규칙을 어기게 만드는 해킹 시도.",
	"description": "이는 LLM 기반 애플리케이션의 보안 취약점 중 하나로, 공격자가 시스템의 보안 경계를 우회하거나, 민감한 데이터를 탈취하는 데 사용될 수 있다. 사용자 입력과 시스템 프롬프트를 명확히 구분하고, 입력의 위험도를 평가하는 방어 로직이 요구된다.",
	"tags": ["AI", "보안/네트워크", "LLM"],
	"primaryTag": "AI",
	"relatedIds": [4010, 4106],
	"confusableIds": [4097],
	"useCases": [
		{
			"role": "Dev",
			"text": "프롬프트 인젝션 방어를 위해 사용자 입력을 시스템 프롬프트와 완전히 격리하고 필터링 로직을 추가해야 합니다."
		},
		{
			"role": "PM",
			"text": "보안 테스트에서 프롬프트 인젝션 취약점이 발견되어, LLM 기반 서비스 배포를 잠정 연기했습니다."
		}
	],
	"keywords": [
		"prompt-injection",
		"프롬프트",
		"인젝션",
		"llm-security",
		"보안"
	],
	"level": "advanced",
	"updatedAt": "2025-11-30"
}
